{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90930093-98d1-4951-8988-5b7b726d9b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import requests\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.chrome.service import Service  # Import Service\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import json\n",
    "import datetime\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50659c8e",
   "metadata": {},
   "source": [
    "# Hard Rock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad2c34c-2acc-411b-bd0c-1fcebe6f5670",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hard Rock ###\n",
    "\n",
    "# ChromeDriver path and Chrome options\n",
    "chromedriver_path = \"/opt/homebrew/bin/chromedriver\"  # Adjust this path based on where your ChromeDriver is installed\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Run Chrome in headless mode\n",
    "service = Service(chromedriver_path)\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "# URL to scrape\n",
    "url = 'https://app.hardrock.bet/home/competition/nfl/2233719185909481580'\n",
    "driver.get(url)\n",
    "\n",
    "# Wait for the page to load completely\n",
    "time.sleep(5)\n",
    "\n",
    "# Define the tabs to click on, these are found using their data-cy attributes or other identifiers\n",
    "tabs = {\n",
    "    \"Game Lines\": \"//div[@data-cy='game-details-tab:Game Lines']\",\n",
    "    \"Player Props\": \"//div[@data-cy='game-details-tab:Player Props']\",\n",
    "    \"Halves\": \"//div[@data-cy='game-details-tab:Halves']\",\n",
    "    \"Quarters\": \"//div[@data-cy='game-details-tab:Quarters']\"\n",
    "}\n",
    "\n",
    "# Initialize the CSV data\n",
    "csv_data = []\n",
    "csv_data.append([\"Tab\", \"Market\", \"Selection\", \"Odds\"])\n",
    "\n",
    "# Loop through each tab, simulate the click, and scrape the data\n",
    "for tab_name, tab_xpath in tabs.items():\n",
    "    tab_element = driver.find_element(By.XPATH, tab_xpath)\n",
    "    \n",
    "    # Use JavaScript to trigger the click\n",
    "    driver.execute_script(\"arguments[0].click();\", tab_element)\n",
    "    \n",
    "    # Wait for the content to load\n",
    "    time.sleep(3)  # Adjust the time based on the complexity of the page\n",
    "    \n",
    "    # Get the page source after clicking and parse it with BeautifulSoup\n",
    "    html_content = driver.page_source\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    \n",
    "    # Find all 'marketView' sections across the page\n",
    "    market_views = soup.find_all(\"div\", class_=\"marketView\")\n",
    "\n",
    "    # Extract and classify data by looking at the current tab section\n",
    "    for market in market_views:\n",
    "        # Extract the market header, selection names, and odds\n",
    "        header = market.find(\"div\", class_=\"marketHeader\").get_text(strip=True) if market.find(\"div\", class_=\"marketHeader\") else \"No Header\"\n",
    "        selections = market.find_all(\"div\", class_=\"selectionName\")\n",
    "        odds = market.find_all(\"div\", class_=\"selection-odds\")\n",
    "        \n",
    "        for selection, odd in zip(selections, odds):\n",
    "            selection_name = selection.get_text(strip=True)\n",
    "            selection_odds = odd.get_text(strip=True)\n",
    "            csv_data.append([tab_name, header, selection_name, selection_odds])\n",
    "\n",
    "# Save the extracted data to a CSV file\n",
    "csv_path = 'hard_rock_props.csv'\n",
    "with open(csv_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(csv_data)\n",
    "\n",
    "print(f\"All tabs' data has been saved to {csv_path}\")\n",
    "\n",
    "# Close the browser after scraping\n",
    "driver.quit()\n",
    "!open hard_rock_props.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24961730",
   "metadata": {},
   "source": [
    "# Bovada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7d4643b1-0789-48de-b016-d0787e613c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved HTML content for Alternate Lines tab.\n",
      "Saved HTML content for TD Scorer Props tab.\n",
      "Saved HTML content for Passing Props tab.\n",
      "Saved HTML content for Receiving Props tab.\n",
      "Saved HTML content for Rushing Props tab.\n"
     ]
    }
   ],
   "source": [
    "### Bovada ###\n",
    "\n",
    "# ChromeDriver path and Chrome options\n",
    "chromedriver_path = \"/opt/homebrew/bin/chromedriver\"  # Adjust this path based on where your ChromeDriver is installed\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Run Chrome in headless mode\n",
    "service = Service(chromedriver_path)\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "# URL to scrape\n",
    "url = 'https://www.bovada.lv/sports/football/nfl/new-york-giants-pittsburgh-steelers-202410282015'\n",
    "driver.get(url)\n",
    "\n",
    "# Wait for the page to load completely\n",
    "time.sleep(5)\n",
    "\n",
    "# Define the tabs to click on, targeting the title attributes of the 'li' elements\n",
    "tabs = {\n",
    "    \"Alternate Lines\": \"//li[@title='Alternate Lines']\",\n",
    "    \"TD Scorer Props\": \"//li[@title='TD Scorer Props']\",\n",
    "    \"Passing Props\": \"//li[@title='Passing Props']\",\n",
    "    \"Receiving Props\": \"//li[@title='Receiving Props']\",\n",
    "    \"Rushing Props\": \"//li[@title='Rushing Props']\"\n",
    "}\n",
    "\n",
    "# Loop through each tab, simulate the click, and save the raw HTML\n",
    "for tab_name, tab_xpath in tabs.items():\n",
    "    try:\n",
    "        # Find the tab element and click on it\n",
    "        tab_element = driver.find_element(By.XPATH, tab_xpath)\n",
    "        driver.execute_script(\"arguments[0].click();\", tab_element)\n",
    "        \n",
    "        # Wait for the content to load\n",
    "        time.sleep(3)  # Adjust the time based on the complexity of the page\n",
    "        \n",
    "        # Get the page source after clicking\n",
    "        html_content = driver.page_source\n",
    "        \n",
    "        # Save the HTML content to a file\n",
    "        with open(f\"data/bovada/{tab_name.replace(' ', '_')}_tab.html\", \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(html_content)\n",
    "        \n",
    "        print(f\"Saved HTML content for {tab_name} tab.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing tab {tab_name}: {e}\")\n",
    "\n",
    "# Close the browser after scraping\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d772cac2-5887-4ecc-99d3-98a855c8d66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 399 props to all_receiving_props.csv\n",
      "\n",
      "First few rows of extracted data:\n",
      "         Prop_Type             Player Team  Line  Over_Odds  Under_Odds\n",
      "0  Receiving Yards  Calvin Austin III  PIT  18.5       -110        -120\n",
      "1  Receiving Yards  Calvin Austin III  PIT  25.5       -110        -120\n",
      "2  Receiving Yards  Calvin Austin III  PIT   8.5       -120        -110\n",
      "3  Receiving Yards  Calvin Austin III  PIT  59.5       -115        -115\n",
      "4  Receiving Yards  Calvin Austin III  PIT  13.5       -110        -120\n"
     ]
    }
   ],
   "source": [
    "# Receiving Props\n",
    "\n",
    "def clean_odds(odds_text):\n",
    "    \"\"\"Clean odds text to standardized format.\"\"\"\n",
    "    if not odds_text:\n",
    "        return None\n",
    "    odds_text = odds_text.strip()\n",
    "    if odds_text == \"EVEN\":\n",
    "        return 100\n",
    "    try:\n",
    "        return int(re.sub(r'[^0-9-]', '', odds_text))\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def extract_all_receiving_props(html_path):\n",
    "    # Read HTML file\n",
    "    with open(html_path, 'r', encoding='utf-8') as file:\n",
    "        soup = BeautifulSoup(file, 'html.parser')\n",
    "\n",
    "    props_data = []\n",
    "\n",
    "    # Loop through all headers to capture all occurrences\n",
    "    headers = soup.find_all('header', {'class': 'game-heading'})\n",
    "\n",
    "    for header in headers:\n",
    "        h3_elem = header.find('h3', {'class': 'league-header full-width'})\n",
    "        if not h3_elem:\n",
    "            continue\n",
    "\n",
    "        market_name = h3_elem.text.strip()\n",
    "\n",
    "        # Find all market containers under the current header\n",
    "        market_containers = header.find_all_next('section', {'class': 'coupon-content markets-container'})\n",
    "        \n",
    "        for market_container in market_containers:\n",
    "            # Extract player and team\n",
    "            match = re.search(r'Total (.+?) - (.+?) \\((.+?)\\)', market_name)\n",
    "            if not match:\n",
    "                continue\n",
    "\n",
    "            prop_type = match.group(1).strip()\n",
    "            player = match.group(2).strip()\n",
    "            team = match.group(3).strip()\n",
    "\n",
    "            # Extract line\n",
    "            spread_header = market_container.find('ul', {'class': 'spread-header'})\n",
    "            line = None\n",
    "            if spread_header and spread_header.find('li'):\n",
    "                try:\n",
    "                    line = float(spread_header.find('li').text.strip())\n",
    "                except ValueError:\n",
    "                    continue\n",
    "\n",
    "            # Extract odds\n",
    "            odds_spans = market_container.find_all('span', {'class': 'bet-price'})\n",
    "            over_odds = clean_odds(odds_spans[0].text) if len(odds_spans) > 0 else None\n",
    "            under_odds = clean_odds(odds_spans[1].text) if len(odds_spans) > 1 else None\n",
    "\n",
    "            props_data.append({\n",
    "                'Prop_Type': prop_type,\n",
    "                'Player': player,\n",
    "                'Team': team,\n",
    "                'Line': line,\n",
    "                'Over_Odds': over_odds,\n",
    "                'Under_Odds': under_odds\n",
    "            })\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(props_data)\n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Extract data\n",
    "        df = extract_all_receiving_props('Receiving_Props_tab.html')\n",
    "        \n",
    "        if df.empty:\n",
    "            print(\"No props were found in the HTML file.\")\n",
    "            return\n",
    "\n",
    "        # Save to CSV\n",
    "        df.to_csv('receiving_props.csv', index=False)\n",
    "        print(f\"Saved {len(df)} props to all_receiving_props.csv\")\n",
    "\n",
    "        # Display first few rows\n",
    "        print(\"\\nFirst few rows of extracted data:\")\n",
    "        print(df.head())\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        raise e\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb1690ad-440c-4ac1-bb1d-00ecb75f6645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 55 props to all_rushing_props.csv\n",
      "\n",
      "First few rows of extracted data:\n",
      "                   Prop_Type         Player Team  Line  Over_Odds  Under_Odds\n",
      "0  Rushing & Receiving Yards  Jaylen Warren  PIT  47.5       -115        -115\n",
      "1  Rushing & Receiving Yards  Jaylen Warren  PIT  65.5       -115        -115\n",
      "2  Rushing & Receiving Yards  Jaylen Warren  PIT  24.5       -115        -115\n",
      "3  Rushing & Receiving Yards  Jaylen Warren  PIT  31.5       -115        -115\n",
      "4  Rushing & Receiving Yards  Jaylen Warren  PIT  30.5       -115        -115\n"
     ]
    }
   ],
   "source": [
    "# Rushing Props\n",
    "\n",
    "def clean_odds(odds_text):\n",
    "    \"\"\"Clean odds text to standardized format.\"\"\"\n",
    "    if not odds_text:\n",
    "        return None\n",
    "    odds_text = odds_text.strip()\n",
    "    if odds_text == \"EVEN\":\n",
    "        return 100\n",
    "    try:\n",
    "        return int(re.sub(r'[^0-9-]', '', odds_text))\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def extract_all_rushing_props(html_path):\n",
    "    # Read HTML file\n",
    "    with open(html_path, 'r', encoding='utf-8') as file:\n",
    "        soup = BeautifulSoup(file, 'html.parser')\n",
    "    \n",
    "    props_data = []\n",
    "    \n",
    "    # Find all prop headers using the correct class structure\n",
    "    headers = soup.find_all('header', {'class': 'game-heading'})\n",
    "    \n",
    "    for header in headers:\n",
    "        # Get market name from h3 element\n",
    "        h3_elem = header.find('h3', {'class': 'league-header full-width'})\n",
    "        if not h3_elem:\n",
    "            continue\n",
    "            \n",
    "        market_name = h3_elem.text.strip()\n",
    "        \n",
    "        # Find all associated market containers for this header\n",
    "        market_containers = header.find_all_next('section', {'class': 'coupon-content markets-container'})\n",
    "        \n",
    "        for market_container in market_containers:\n",
    "            # Extract player name and team\n",
    "            match = re.search(r'Total (.+?) - (.+?) \\((.+?)\\)', market_name)\n",
    "            if not match:\n",
    "                continue\n",
    "            \n",
    "            prop_type = match.group(1).strip()\n",
    "            player = match.group(2).strip()\n",
    "            team = match.group(3).strip()\n",
    "            \n",
    "            # Extract line \n",
    "            spread_header = market_container.find('ul', {'class': 'spread-header'})\n",
    "            line = None\n",
    "            if spread_header and spread_header.find('li'):\n",
    "                try:\n",
    "                    line = float(spread_header.find('li').text.strip())\n",
    "                except ValueError:\n",
    "                    continue\n",
    "            \n",
    "            # Find odds values - look for bet-price spans\n",
    "            odds_spans = market_container.find_all('span', {'class': 'bet-price'})\n",
    "            over_odds = clean_odds(odds_spans[0].text) if len(odds_spans) > 0 else None\n",
    "            under_odds = clean_odds(odds_spans[1].text) if len(odds_spans) > 1 else None\n",
    "            \n",
    "            props_data.append({\n",
    "                'Prop_Type': prop_type,\n",
    "                'Player': player,\n",
    "                'Team': team,\n",
    "                'Line': line,\n",
    "                'Over_Odds': over_odds,\n",
    "                'Under_Odds': under_odds\n",
    "            })\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(props_data)\n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Extract data\n",
    "        df = extract_all_rushing_props('Rushing_Props_tab.html')\n",
    "        \n",
    "        if df.empty:\n",
    "            print(\"No props were found in the HTML file.\")\n",
    "            return\n",
    "            \n",
    "        # Save to CSV\n",
    "        csv_path = 'rushing_props.csv'\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        print(f\"Saved {len(df)} props to all_rushing_props.csv\")\n",
    "        \n",
    "        # Display first few rows\n",
    "        print(\"\\nFirst few rows of extracted data:\")\n",
    "        print(df.head())\n",
    "        return csv_path, df.head()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        raise e\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f256d19-379f-48a3-b589-82e6ced495a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 75 props to all_props.csv\n",
      "\n",
      "First few rows of extracted data:\n",
      "       Prop_Type        Player Team   Line  Over_Odds  Under_Odds\n",
      "0  Passing Yards  Daniel Jones  NYG  187.5       -115        -115\n",
      "1  Passing Yards  Daniel Jones  NYG  201.5       -115        -115\n",
      "2  Passing Yards  Daniel Jones  NYG    0.5       -145         110\n",
      "3  Passing Yards  Daniel Jones  NYG    1.5        155        -210\n",
      "4  Passing Yards  Daniel Jones  NYG   19.5        105        -135\n"
     ]
    }
   ],
   "source": [
    "# Passing Props\n",
    "\n",
    "def clean_odds(odds_text):\n",
    "    \"\"\"Clean odds text to standardized format.\"\"\"\n",
    "    if not odds_text:\n",
    "        return None\n",
    "    odds_text = odds_text.strip()\n",
    "    if odds_text == \"EVEN\":\n",
    "        return 100\n",
    "    try:\n",
    "        return int(re.sub(r'[^0-9-]', '', odds_text))\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def extract_all_props(html_path):\n",
    "    # Read HTML file\n",
    "    with open(html_path, 'r', encoding='utf-8') as file:\n",
    "        soup = BeautifulSoup(file, 'html.parser')\n",
    "\n",
    "    props_data = []\n",
    "\n",
    "    # Find all prop headers using the correct class structure\n",
    "    headers = soup.find_all('header', {'class': 'game-heading'})\n",
    "\n",
    "    for header in headers:\n",
    "        # Get market name from h3 element\n",
    "        h3_elem = header.find('h3', {'class': 'league-header full-width'})\n",
    "        if not h3_elem:\n",
    "            continue\n",
    "\n",
    "        market_name = h3_elem.text.strip()\n",
    "\n",
    "        # Find all associated market containers for this header\n",
    "        market_containers = header.find_all_next('section', {'class': 'coupon-content markets-container'})\n",
    "\n",
    "        for market_container in market_containers:\n",
    "            # Extract player name and team\n",
    "            match = re.search(r'Total (.+?) - (.+?) \\((.+?)\\)', market_name)\n",
    "            if not match:\n",
    "                continue\n",
    "\n",
    "            prop_type = match.group(1).strip()\n",
    "            player = match.group(2).strip()\n",
    "            team = match.group(3).strip()\n",
    "\n",
    "            # Extract line\n",
    "            spread_header = market_container.find('ul', {'class': 'spread-header'})\n",
    "            line = None\n",
    "            if spread_header and spread_header.find('li'):\n",
    "                try:\n",
    "                    line = float(spread_header.find('li').text.strip())\n",
    "                except ValueError:\n",
    "                    continue\n",
    "\n",
    "            # Find odds values - look for bet-price spans\n",
    "            odds_spans = market_container.find_all('span', {'class': 'bet-price'})\n",
    "            over_odds = clean_odds(odds_spans[0].text) if len(odds_spans) > 0 else None\n",
    "            under_odds = clean_odds(odds_spans[1].text) if len(odds_spans) > 1 else None\n",
    "\n",
    "            props_data.append({\n",
    "                'Prop_Type': prop_type,\n",
    "                'Player': player,\n",
    "                'Team': team,\n",
    "                'Line': line,\n",
    "                'Over_Odds': over_odds,\n",
    "                'Under_Odds': under_odds\n",
    "            })\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(props_data)\n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Extract data\n",
    "        df = extract_all_props('Passing_Props_tab.html')\n",
    "        \n",
    "        if df.empty:\n",
    "            print(\"No props were found in the HTML file.\")\n",
    "            return\n",
    "\n",
    "        # Save to CSV\n",
    "        df.to_csv('passing_props.csv', index=False)\n",
    "        print(f\"Saved {len(df)} props to all_props.csv\")\n",
    "\n",
    "        # Display first few rows\n",
    "        print(\"\\nFirst few rows of extracted data:\")\n",
    "        print(df.head())\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        raise e\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80bb7b4a-f95c-4d96-9450-0200ba8b9de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 12 alternate lines to alternate_lines.csv\n",
      "\n",
      "First few rows of extracted data:\n",
      "                      Market_Name  Line  Over_Odds  Under_Odds\n",
      "0                          Spread   NaN        120         110\n",
      "1            Spread  - First Half   NaN        130         120\n",
      "2           Spread  - 1st Quarter   NaN       -135         105\n",
      "3                    Total Points  34.0       -160        -150\n",
      "4  Total Points - New York Giants  14.5       -105        -125\n"
     ]
    }
   ],
   "source": [
    "# Alternate Lines\n",
    "\n",
    "def clean_odds(odds_text):\n",
    "    if not odds_text:\n",
    "        return None\n",
    "    odds_text = odds_text.strip()\n",
    "    if odds_text == \"EVEN\":\n",
    "        return 100\n",
    "    try:\n",
    "        return int(re.sub(r'[^0-9-]', '', odds_text))\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def extract_all_alternate_lines(html_path):\n",
    "    with open(html_path, 'r', encoding='utf-8') as file:\n",
    "        soup = BeautifulSoup(file, 'html.parser')\n",
    "\n",
    "    lines_data = []\n",
    "\n",
    "    headers = soup.find_all('header', {'class': 'game-heading'})\n",
    "\n",
    "    for header in headers:\n",
    "        h3_elem = header.find('h3', {'class': 'league-header full-width'})\n",
    "        if not h3_elem:\n",
    "            continue\n",
    "\n",
    "        market_name = h3_elem.text.strip()\n",
    "\n",
    "        market_containers = header.find_next_siblings('section', {'class': 'coupon-content markets-container'})\n",
    "        \n",
    "        for market_container in market_containers:\n",
    "            spread_header = market_container.find('ul', {'class': 'spread-header'})\n",
    "            line = None\n",
    "            if spread_header and spread_header.find('li'):\n",
    "                try:\n",
    "                    line = float(spread_header.find('li').text.strip())\n",
    "                except ValueError:\n",
    "                    continue\n",
    "\n",
    "            odds_spans = market_container.find_all('span', {'class': 'bet-price'})\n",
    "            over_odds = clean_odds(odds_spans[0].text) if len(odds_spans) > 0 else None\n",
    "            under_odds = clean_odds(odds_spans[1].text) if len(odds_spans) > 1 else None\n",
    "\n",
    "            lines_data.append({\n",
    "                'Market_Name': market_name,\n",
    "                'Line': line,\n",
    "                'Over_Odds': over_odds,\n",
    "                'Under_Odds': under_odds\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(lines_data)\n",
    "    \n",
    "    # Save to CSV file\n",
    "    df.to_csv('alternate_lines.csv', index=False)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        df = extract_all_alternate_lines('Alternate_Lines_tab.html')\n",
    "        \n",
    "        if df.empty:\n",
    "            print(\"No alternate lines were found in the HTML file.\")\n",
    "            return\n",
    "\n",
    "        print(f\"Saved {len(df)} alternate lines to alternate_lines.csv\")\n",
    "\n",
    "        print(\"\\nFirst few rows of extracted data:\")\n",
    "        print(df.head())\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        raise e\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1c792d2b-dfcf-4301-a4d5-403d872cd647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to data/bovada/td_scorer_props.csv\n",
      "Refined TD Scorer Props Data:\n",
      "                       Player    Odds\n",
      "0         No Touchdown Scorer    -600\n",
      "1          Malik Nabers (NYG)   +1400\n",
      "2      Devin Singletary (NYG)   +1800\n",
      "3      Tyrone Tracy Jr. (NYG)   +1800\n",
      "4     Wan'dale Robinson (NYG)   +2000\n",
      "..                        ...     ...\n",
      "99     Daniel Bellinger (NYG)  +17500\n",
      "100           Eric Gray (NYG)  +17500\n",
      "101      Chris Manhertz (NYG)  +20000\n",
      "102     Rodney Williams (PIT)  +20000\n",
      "103  Bryce Ford-Wheaton (NYG)  +25000\n",
      "\n",
      "[104 rows x 2 columns]\n",
      "\n",
      "First few rows of the DataFrame:\n",
      "                    Player   Odds\n",
      "0      No Touchdown Scorer   -600\n",
      "1       Malik Nabers (NYG)  +1400\n",
      "2   Devin Singletary (NYG)  +1800\n",
      "3   Tyrone Tracy Jr. (NYG)  +1800\n",
      "4  Wan'dale Robinson (NYG)  +2000\n"
     ]
    }
   ],
   "source": [
    "# Touchdown Scorer Props\n",
    "\n",
    "def extract_td_scorer_props(html_path):\n",
    "    # Read HTML file\n",
    "    with open(html_path, 'r', encoding='utf-8') as file:\n",
    "        soup = BeautifulSoup(file, 'html.parser')\n",
    "\n",
    "    # Initialize a list to store the TD Scorer Props data\n",
    "    td_scorer_props_data_v2 = []\n",
    "\n",
    "    # Loop through all relevant sections for outcomes and odds\n",
    "    for outcome in soup.find_all(\"button\", class_=\"bet-btn\"):\n",
    "        # Extract player name correctly\n",
    "        player_name = outcome.find(\"span\", class_=\"outcomes\").get_text(strip=True) if outcome.find(\"span\", class_=\"outcomes\") else None\n",
    "        \n",
    "        # Extract the odds\n",
    "        odds = outcome.find(\"span\", class_=\"bet-price\").get_text(strip=True) if outcome.find(\"span\", class_=\"bet-price\") else None\n",
    "\n",
    "        # Append only if both player name and odds are found\n",
    "        if player_name and odds:\n",
    "            td_scorer_props_data_v2.append([player_name, odds])\n",
    "\n",
    "    # Convert the list to a pandas DataFrame\n",
    "    df_td_scorer_props_final_v2 = pd.DataFrame(td_scorer_props_data_v2, columns=[\"Player\", \"Odds\"])\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    csv_filename = 'data/bovada/td_scorer_props.csv'\n",
    "    df_td_scorer_props_final_v2.to_csv(csv_filename, index=False)\n",
    "    print(f\"Data saved to {csv_filename}\")\n",
    "\n",
    "    # Display the DataFrame using pandas\n",
    "    print(\"Refined TD Scorer Props Data:\")\n",
    "    print(df_td_scorer_props_final_v2)\n",
    "\n",
    "    # Show the first few rows\n",
    "    return df_td_scorer_props_final_v2.head()\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    df_head = extract_td_scorer_props('data/bovada/TD_Scorer_Props_tab.html')\n",
    "    print(\"\\nFirst few rows of the DataFrame:\")\n",
    "    print(df_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2afa0676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to dst_props.csv\n",
      "D/ST Props Data:\n",
      "                                              Player Over Value Over Odds  \\\n",
      "0  Total Tackles and Assists - Cameron Heyward (PIT)        3.5      -115   \n",
      "1     Total Tackles and Assists - Dexter Lawrence II        3.5      +125   \n",
      "2    Total Tackles and Assists - Jason Pinnock (NYG)        4.5      -160   \n",
      "3        Total Tackles and Assists - Joey Porter Jr.        3.5      -160   \n",
      "4  Total Tackles and Assists - Minkah Fitzpatrick...        6.5      -105   \n",
      "5    Total Tackles and Assists - Patrick Queen (PIT)        6.5      -145   \n",
      "6        Total Tackles and Assists - T.J. Watt (PIT)        3.5      -120   \n",
      "7      Total Tackles and Assists - Tyler Nubin (NYG)        6.5      -135   \n",
      "8         Total Kicking Points - Chris Boswell (PIT)        6.5      -135   \n",
      "9            Total Kicking Points - Greg Joseph (GB)        5.5      -115   \n",
      "\n",
      "  Under Odds  \n",
      "0       -115  \n",
      "1       -165  \n",
      "2       +120  \n",
      "3       +120  \n",
      "4       -125  \n",
      "5       +110  \n",
      "6       -110  \n",
      "7       +105  \n",
      "8       +105  \n",
      "9       -115  \n",
      "\n",
      "First few rows of the DataFrame:\n",
      "                                              Player Over Value Over Odds  \\\n",
      "0  Total Tackles and Assists - Cameron Heyward (PIT)        3.5      -115   \n",
      "1     Total Tackles and Assists - Dexter Lawrence II        3.5      +125   \n",
      "2    Total Tackles and Assists - Jason Pinnock (NYG)        4.5      -160   \n",
      "3        Total Tackles and Assists - Joey Porter Jr.        3.5      -160   \n",
      "4  Total Tackles and Assists - Minkah Fitzpatrick...        6.5      -105   \n",
      "\n",
      "  Under Odds  \n",
      "0       -115  \n",
      "1       -165  \n",
      "2       +120  \n",
      "3       +120  \n",
      "4       -125  \n"
     ]
    }
   ],
   "source": [
    "# D/ST Props\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to extract D/ST props\n",
    "def extract_dst_props(html_path):\n",
    "    # Read HTML file\n",
    "    with open(html_path, 'r', encoding='utf-8') as file:\n",
    "        soup = BeautifulSoup(file, 'html.parser')\n",
    "\n",
    "    # Initialize a list to store the D/ST Props data\n",
    "    dst_props_data = []\n",
    "\n",
    "    # Loop through all relevant sections for player names and odds\n",
    "    for section in soup.find_all(\"section\", class_=\"coupon-content markets-container\"):\n",
    "        # Find the player name\n",
    "        player_header = section.find_previous(\"h3\")\n",
    "        player_name = player_header.get_text(strip=True) if player_header else None\n",
    "\n",
    "        # Extract over and under values\n",
    "        spread_header = section.find(\"ul\", class_=\"spread-header\")\n",
    "        over_value = spread_header.find(\"li\").get_text(strip=True) if spread_header else None\n",
    "\n",
    "        # Extract the odds for over and under\n",
    "        market_types = section.find_all(\"ul\", class_=\"market-type\")\n",
    "        if len(market_types) >= 2:\n",
    "            over_odds = market_types[0].find(\"span\", class_=\"bet-price\").get_text(strip=True) if market_types[0].find(\"span\", class_=\"bet-price\") else None\n",
    "            under_odds = market_types[1].find(\"span\", class_=\"bet-price\").get_text(strip=True) if market_types[1].find(\"span\", class_=\"bet-price\") else None\n",
    "\n",
    "            # Append the data if all values are available\n",
    "            if player_name and over_value and over_odds and under_odds:\n",
    "                dst_props_data.append([player_name, over_value, over_odds, under_odds])\n",
    "\n",
    "    # Convert the list to a pandas DataFrame\n",
    "    df_dst_props = pd.DataFrame(dst_props_data, columns=[\"Player\", \"Over Value\", \"Over Odds\", \"Under Odds\"])\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    csv_filename = 'dst_props.csv'\n",
    "    df_dst_props.to_csv(csv_filename, index=False)\n",
    "    print(f\"Data saved to {csv_filename}\")\n",
    "\n",
    "    # Display the DataFrame using pandas\n",
    "    print(\"D/ST Props Data:\")\n",
    "    print(df_dst_props)\n",
    "\n",
    "    # Show the first few rows\n",
    "    return df_dst_props.head()\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    df_head = extract_dst_props('D_ST_Props_tab.html')\n",
    "    print(\"\\nFirst few rows of the DataFrame:\")\n",
    "    print(df_head)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd640147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Touchdown Props\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to extract Touchdown Scorer props\n",
    "def extract_td_scorer_props(html_path):\n",
    "    # Read HTML file\n",
    "    with open(html_path, 'r', encoding='utf-8') as file:\n",
    "        soup = BeautifulSoup(file, 'html.parser')\n",
    "\n",
    "    # Initialize a list to store the Touchdown Scorer Props data\n",
    "    td_scorer_props_data = []\n",
    "\n",
    "    # Loop through all 'sp-outcome' tags that contain the props data\n",
    "    for outcome in soup.find_all(\"sp-outcome\"):\n",
    "        # Extract player name from the 'span' with class 'outcomes'\n",
    "        player_name = outcome.find(\"span\", class_=\"outcomes\").get_text(strip=True) if outcome.find(\"span\", class_=\"outcomes\") else None\n",
    "        \n",
    "        # Extract the odds from the 'span' with class 'bet-price'\n",
    "        td_odds = outcome.find(\"span\", class_=\"bet-price\").get_text(strip=True) if outcome.find(\"span\", class_=\"bet-price\") else None\n",
    "\n",
    "        # Append only if both player name and odds are found\n",
    "        if player_name and td_odds:\n",
    "            td_scorer_props_data.append([player_name, td_odds])\n",
    "\n",
    "    # Convert the list to a pandas DataFrame\n",
    "    df_td_scorer_props = pd.DataFrame(td_scorer_props_data, columns=[\"Player\", \"TD Scorer Odds\"])\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    csv_filename = 'td_scorer_props.csv'\n",
    "    df_td_scorer_props.to_csv(csv_filename, index=False)\n",
    "    print(f\"Data saved to {csv_filename}\")\n",
    "\n",
    "    # Display the DataFrame using pandas\n",
    "    print(\"Touchdown Scorer Props Data:\")\n",
    "    print(df_td_scorer_props)\n",
    "\n",
    "    # Show the first few rows\n",
    "    return df_td_scorer_props.head()\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    df_head = extract_td_scorer_props('Touchdown_Props_tab.html')\n",
    "    print(\"\\nFirst few rows of the DataFrame:\")\n",
    "    print(df_head)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b86497e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parlays\n",
    "# Score Props\n",
    "# Correct Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "900f135b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to special_bets.csv\n",
      "Special Bets Data:\n",
      "                                      Bet Description    Odds\n",
      "0   Najee Harris to record 25+ Rushing Yards in Ea...    +140\n",
      "1   Malik Nabers to record 25+ Receiving Yards in ...    +150\n",
      "2                                  u30.5 Total Points    +200\n",
      "3   George Pickens to record 25+ Receiving Yards i...    +220\n",
      "4              Both teams to score 1+ TD in each half    +390\n",
      "5   George Pickens and Malik Nabers to Each have 2...    +400\n",
      "6                     Russell Wilson 2+ Interceptions    +475\n",
      "7                                  u23.5 Total Points    +600\n",
      "8   Each team to score 1+ rushing TDs & 1+ passing...    +625\n",
      "9   George Pickens & Malik Nabers to Each Record 5...   +1500\n",
      "10  D.Jones 192+ Pass Yds, R.Wilson 202+ Pass Yds,...   +1800\n",
      "11  Russell Wilson & Daniel Jones 2+ Interceptions...   +3000\n",
      "12      Each team to score 1+ TD & 1+ FG in each half   +3300\n",
      "13  W.Robinson 50+ Receiving Yards, G.Pickens 75+ ...   +4000\n",
      "14             Both teams to score 2+ TD in each half  +10000\n",
      "15  Russell Wilson to record 1+ Passing TD in Each...  +12500\n",
      "\n",
      "First few rows of the DataFrame:\n",
      "                                     Bet Description  Odds\n",
      "0  Najee Harris to record 25+ Rushing Yards in Ea...  +140\n",
      "1  Malik Nabers to record 25+ Receiving Yards in ...  +150\n",
      "2                                 u30.5 Total Points  +200\n",
      "3  George Pickens to record 25+ Receiving Yards i...  +220\n",
      "4             Both teams to score 1+ TD in each half  +390\n"
     ]
    }
   ],
   "source": [
    "# Special Bets\n",
    "\n",
    "# Function to extract Special Bets\n",
    "def extract_special_bets(html_path):\n",
    "    # Read HTML file\n",
    "    with open(html_path, 'r', encoding='utf-8') as file:\n",
    "        soup = BeautifulSoup(file, 'html.parser')\n",
    "\n",
    "    # Initialize a list to store the Special Bets data\n",
    "    special_bets_data = []\n",
    "\n",
    "    # Loop through all 'sp-outcome' tags that contain special bet data\n",
    "    for outcome in soup.find_all(\"sp-outcome\"):\n",
    "        # Extract description of the bet from the 'span' with class 'outcomes'\n",
    "        bet_description = outcome.find(\"span\", class_=\"outcomes\").get_text(strip=True) if outcome.find(\"span\", class_=\"outcomes\") else None\n",
    "        \n",
    "        # Extract the odds from the 'span' with class 'bet-price'\n",
    "        bet_odds = outcome.find(\"span\", class_=\"bet-price\").get_text(strip=True) if outcome.find(\"span\", class_=\"bet-price\") else None\n",
    "\n",
    "        # Append the data if both bet description and odds are found\n",
    "        if bet_description and bet_odds:\n",
    "            special_bets_data.append([bet_description, bet_odds])\n",
    "\n",
    "    # Convert the list to a pandas DataFrame\n",
    "    df_special_bets = pd.DataFrame(special_bets_data, columns=[\"Bet Description\", \"Odds\"])\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    csv_filename = 'special_bets.csv'\n",
    "    df_special_bets.to_csv(csv_filename, index=False)\n",
    "    print(f\"Data saved to {csv_filename}\")\n",
    "\n",
    "    # Display the DataFrame using pandas\n",
    "    print(\"Special Bets Data:\")\n",
    "    print(df_special_bets)\n",
    "\n",
    "    # Show the first few rows\n",
    "    return df_special_bets.head()\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    df_head = extract_special_bets('Special_Bets_tab.html')\n",
    "    print(\"\\nFirst few rows of the DataFrame:\")\n",
    "    print(df_head)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b28607c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to game_props.csv\n",
      "Game Props Data:\n",
      "            Prop Description   Odds\n",
      "0       Giants by 1-6 points   +385\n",
      "1     Steelers by 1-6 points   +260\n",
      "2      Giants by 7-12 points   +700\n",
      "3    Steelers by 7-12 points   +320\n",
      "4     Giants by 13-18 points  +1200\n",
      "..                       ...    ...\n",
      "96                  Odd - 1H   -115\n",
      "97                 Even - 1Q   -110\n",
      "98                  Odd - 1Q   -120\n",
      "99                 Yes - REG   +900\n",
      "100                 No - REG  -3300\n",
      "\n",
      "[101 rows x 2 columns]\n",
      "\n",
      "First few rows of the DataFrame:\n",
      "          Prop Description   Odds\n",
      "0     Giants by 1-6 points   +385\n",
      "1   Steelers by 1-6 points   +260\n",
      "2    Giants by 7-12 points   +700\n",
      "3  Steelers by 7-12 points   +320\n",
      "4   Giants by 13-18 points  +1200\n"
     ]
    }
   ],
   "source": [
    "# Game Props\n",
    "\n",
    "# Function to extract Game Props\n",
    "def extract_game_props(html_path):\n",
    "    # Read HTML file\n",
    "    with open(html_path, 'r', encoding='utf-8') as file:\n",
    "        soup = BeautifulSoup(file, 'html.parser')\n",
    "\n",
    "    # Initialize a list to store the Game Props data\n",
    "    game_props_data = []\n",
    "\n",
    "    # Loop through all 'sp-outcome' tags that contain game prop data\n",
    "    for outcome in soup.find_all(\"sp-outcome\"):\n",
    "        # Extract description of the prop from the 'span' with class 'outcomes'\n",
    "        prop_description = outcome.find(\"span\", class_=\"outcomes\").get_text(strip=True) if outcome.find(\"span\", class_=\"outcomes\") else None\n",
    "        \n",
    "        # Extract the odds from the 'span' with class 'bet-price'\n",
    "        prop_odds = outcome.find(\"span\", class_=\"bet-price\").get_text(strip=True) if outcome.find(\"span\", class_=\"bet-price\") else None\n",
    "\n",
    "        # Append the data if both prop description and odds are found\n",
    "        if prop_description and prop_odds:\n",
    "            game_props_data.append([prop_description, prop_odds])\n",
    "\n",
    "    # Convert the list to a pandas DataFrame\n",
    "    df_game_props = pd.DataFrame(game_props_data, columns=[\"Prop Description\", \"Odds\"])\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    csv_filename = 'game_props.csv'\n",
    "    df_game_props.to_csv(csv_filename, index=False)\n",
    "    print(f\"Data saved to {csv_filename}\")\n",
    "\n",
    "    # Display the DataFrame using pandas\n",
    "    print(\"Game Props Data:\")\n",
    "    print(df_game_props)\n",
    "\n",
    "    # Show the first few rows\n",
    "    return df_game_props.head()\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    df_head = extract_game_props('Game_Props_tab.html')\n",
    "    print(\"\\nFirst few rows of the DataFrame:\")\n",
    "    print(df_head)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cb9e3dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to score_props.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prop Description</th>\n",
       "      <th>Odds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>New York Giants</td>\n",
       "      <td>+130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pittsburgh Steelers</td>\n",
       "      <td>-170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yes - 1H</td>\n",
       "      <td>-475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No - 1H</td>\n",
       "      <td>+320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yes - 1Q</td>\n",
       "      <td>+235</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Prop Description  Odds\n",
       "0      New York Giants  +130\n",
       "1  Pittsburgh Steelers  -170\n",
       "2             Yes - 1H  -475\n",
       "3              No - 1H  +320\n",
       "4             Yes - 1Q  +235"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score Props\n",
    "\n",
    "# Now let's create and run two scripts for the Score Props and Parlays, using the structure identified from the previous scripts.\n",
    "\n",
    "# Function to extract Score Props\n",
    "def extract_score_props(html_path):\n",
    "    # Read HTML file\n",
    "    with open(html_path, 'r', encoding='utf-8') as file:\n",
    "        soup = BeautifulSoup(file, 'html.parser')\n",
    "\n",
    "    # Initialize a list to store the Score Props data\n",
    "    score_props_data = []\n",
    "\n",
    "    # Loop through all 'sp-outcome' tags that contain score prop data\n",
    "    for outcome in soup.find_all(\"sp-outcome\"):\n",
    "        # Extract description of the prop from the 'span' with class 'outcomes'\n",
    "        prop_description = outcome.find(\"span\", class_=\"outcomes\").get_text(strip=True) if outcome.find(\"span\", class_=\"outcomes\") else None\n",
    "        \n",
    "        # Extract the odds from the 'span' with class 'bet-price'\n",
    "        prop_odds = outcome.find(\"span\", class_=\"bet-price\").get_text(strip=True) if outcome.find(\"span\", class_=\"bet-price\") else None\n",
    "\n",
    "        # Append the data if both prop description and odds are found\n",
    "        if prop_description and prop_odds:\n",
    "            score_props_data.append([prop_description, prop_odds])\n",
    "\n",
    "    # Convert the list to a pandas DataFrame\n",
    "    df_score_props = pd.DataFrame(score_props_data, columns=[\"Prop Description\", \"Odds\"])\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    csv_filename = 'score_props.csv'\n",
    "    df_score_props.to_csv(csv_filename, index=False)\n",
    "    print(f\"Data saved to {csv_filename}\")\n",
    "\n",
    "    # Return the DataFrame to show the first few rows\n",
    "    return df_score_props.head()\n",
    "\n",
    "# Running the function for Score Props\n",
    "df_head_score_props = extract_score_props('Score_Props_tab.html')\n",
    "df_head_score_props\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c34f9a74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Parlay Description</th>\n",
       "      <th>Odds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PIT Steelers Punt - NY Giants Punt</td>\n",
       "      <td>+165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PIT Steelers Touchdown - NY Giants Punt</td>\n",
       "      <td>+700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PIT Steelers Field Goal Attempt - NY Giants Punt</td>\n",
       "      <td>+800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PIT Steelers Punt - NY Giants Field Goal Attempt</td>\n",
       "      <td>+850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PIT Steelers Punt - NY Giants Touchdown</td>\n",
       "      <td>+900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Parlay Description  Odds\n",
       "0                PIT Steelers Punt - NY Giants Punt  +165\n",
       "1           PIT Steelers Touchdown - NY Giants Punt  +700\n",
       "2  PIT Steelers Field Goal Attempt - NY Giants Punt  +800\n",
       "3  PIT Steelers Punt - NY Giants Field Goal Attempt  +850\n",
       "4           PIT Steelers Punt - NY Giants Touchdown  +900"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parlays\n",
    "\n",
    "# Function to extract Parlays\n",
    "def extract_parlays(html_path):\n",
    "    # Read HTML file\n",
    "    with open(html_path, 'r', encoding='utf-8') as file:\n",
    "        soup = BeautifulSoup(file, 'html.parser')\n",
    "\n",
    "    # Initialize a list to store the Parlays data\n",
    "    parlays_data = []\n",
    "\n",
    "    # Loop through all 'sp-outcome' tags that contain parlay data\n",
    "    for outcome in soup.find_all(\"sp-outcome\"):\n",
    "        # Extract description of the parlay from the 'span' with class 'outcomes'\n",
    "        parlay_description = outcome.find(\"span\", class_=\"outcomes\").get_text(strip=True) if outcome.find(\"span\", class_=\"outcomes\") else None\n",
    "        \n",
    "        # Extract the odds from the 'span' with class 'bet-price'\n",
    "        parlay_odds = outcome.find(\"span\", class_=\"bet-price\").get_text(strip=True) if outcome.find(\"span\", class_=\"bet-price\") else None\n",
    "\n",
    "        # Append the data if both parlay description and odds are found\n",
    "        if parlay_description and parlay_odds:\n",
    "            parlays_data.append([parlay_description, parlay_odds])\n",
    "\n",
    "    # Convert the list to a pandas DataFrame\n",
    "    df_parlays = pd.DataFrame(parlays_data, columns=[\"Parlay Description\", \"Odds\"])\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    csv_filename = 'parlays.csv'\n",
    "    df_parlays.to_csv(csv_filename, index=False)\n",
    "    print(f\"Data saved to {csv_filename}\")\n",
    "\n",
    "    # Return the DataFrame to show the first few rows\n",
    "    return df_parlays.head()\n",
    "\n",
    "# Running the function for Parlays\n",
    "df_head_parlays = extract_parlays('Parlays_tab.html')\n",
    "df_head_parlays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0f4d8b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to correct_score.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score Description</th>\n",
       "      <th>Odds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Steelers 15 or less and Giants between 16 and 22</td>\n",
       "      <td>+750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Steelers 15 or less and Giants 15 or less</td>\n",
       "      <td>+550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Steelers between 16 and 22 and Giants 15 or less</td>\n",
       "      <td>+400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Steelers 15 or less and Giants 23 or more</td>\n",
       "      <td>+900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Steelers between 16 and 22 and Giants between ...</td>\n",
       "      <td>+725</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Score Description  Odds\n",
       "0   Steelers 15 or less and Giants between 16 and 22  +750\n",
       "1          Steelers 15 or less and Giants 15 or less  +550\n",
       "2   Steelers between 16 and 22 and Giants 15 or less  +400\n",
       "3          Steelers 15 or less and Giants 23 or more  +900\n",
       "4  Steelers between 16 and 22 and Giants between ...  +725"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Correct Score\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Function to extract and save Correct Score Props to CSV\n",
    "def extract_correct_score_props_to_csv(html_path, output_csv):\n",
    "    # Read HTML file\n",
    "    with open(html_path, 'r', encoding='utf-8') as file:\n",
    "        soup = BeautifulSoup(file, 'html.parser')\n",
    "\n",
    "    # Initialize a list to store the Correct Score Props data\n",
    "    correct_score_data = []\n",
    "\n",
    "    # Loop through all 'sp-outcome' tags that contain correct score data\n",
    "    for outcome in soup.find_all(\"sp-outcome\"):\n",
    "        # Extract description of the prop from the 'span' with class 'outcomes'\n",
    "        score_description = outcome.find(\"span\", class_=\"outcomes\").get_text(strip=True) if outcome.find(\"span\", class_=\"outcomes\") else None\n",
    "        \n",
    "        # Extract the odds from the 'span' with class 'bet-price'\n",
    "        score_odds = outcome.find(\"span\", class_=\"bet-price\").get_text(strip=True) if outcome.find(\"span\", class_=\"bet-price\") else None\n",
    "\n",
    "        # Append the data if both description and odds are found\n",
    "        if score_description and score_odds:\n",
    "            correct_score_data.append([score_description, score_odds])\n",
    "\n",
    "    # Convert the list to a pandas DataFrame\n",
    "    df_correct_score = pd.DataFrame(correct_score_data, columns=[\"Score Description\", \"Odds\"])\n",
    "\n",
    "    # Check if a directory is specified, and create it if necessary\n",
    "    output_dir = os.path.dirname(output_csv)\n",
    "    if output_dir:  # Only attempt to create a directory if a directory is specified\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df_correct_score.to_csv(output_csv, index=False)\n",
    "    print(f\"Data saved to {output_csv}\")\n",
    "\n",
    "    # Return the first few rows of the DataFrame\n",
    "    return df_correct_score.head()\n",
    "\n",
    "# Example usage\n",
    "correct_score_csv_path = 'correct_score.csv'  # or use 'data/correct_score.csv' to specify a directory\n",
    "extract_correct_score_props_to_csv('Correct_Score_tab.html', correct_score_csv_path)\n",
    "\n",
    "# df_head_correct_score  # Show the first few rows of the DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f1b73a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a69d03b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea89ffcb",
   "metadata": {},
   "source": [
    "# Stake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac6817b",
   "metadata": {},
   "outputs": [],
   "source": [
    "/html/body/div[1]/div[2]/div[3]/div[3]/div/div[1]/div/div/div[2]/div/div[1]/div/div/div/div/div[2]/div/div/div/div/div/div[4]\n",
    "/html/body/div[1]/div[2]/div[3]/div[3]/div/div[1]/div/div/div[2]/div/div[1]/div/div/div/div/div[2]/div/div/div/div/div/div[4]/div[3]\n",
    "/html/body/div[1]/div[2]/div[3]/div[3]/div/div[1]/div/div/div[2]/div/div[1]/div/div/div/div/div[2]/div/div/div/div/div/div[4]/div[3]/div\n",
    "/html/body/div[1]/div[2]/div[3]/div[3]/div/div[1]/div/div/div[2]/div/div[1]/div/div/div/div/div[2]/div/div/div/div/div/div[4]/div[1]/div/div/button[1] # Main button\n",
    "\n",
    "# from bs4 import BeautifulSoup\n",
    "# import pandas as pd\n",
    "\n",
    "# # Load the local HTML file\n",
    "# html_file_path = '/mnt/data/Pittsburgh Steelers VS New York Giants - Stake.com.html'\n",
    "\n",
    "# with open(html_file_path, 'r', encoding='utf-8') as file:\n",
    "#     html_content = file.read()\n",
    "\n",
    "# # Parse the HTML content with BeautifulSoup\n",
    "# soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# # Navigating through layers in the HTML as per the user's instructions\n",
    "# content_wrapper = soup.find('div', class_='content-wrapper')\n",
    "\n",
    "# if content_wrapper:\n",
    "#     # Dive into the nested structure for 'page-content' -> 'layout-spacing' -> 'secondary-accordion'\n",
    "#     page_content = content_wrapper.find('div', id='main-content')\n",
    "#     layout_spacing = page_content.find('div', class_='layout-spacing') if page_content else None\n",
    "#     accordions = layout_spacing.find_all('div', class_='secondary-accordion') if layout_spacing else []\n",
    "\n",
    "#     # Collect all text data from the accordions\n",
    "#     accordion_data = [accordion.text.strip() for accordion in accordions]\n",
    "\n",
    "#     # Convert to DataFrame\n",
    "#     df = pd.DataFrame(accordion_data, columns=['Accordion Content'])\n",
    "\n",
    "#     # Display the first few rows\n",
    "#     df.head()\n",
    "\n",
    "# else:\n",
    "#     accordion_data = []\n",
    "#     df = pd.DataFrame()\n",
    "\n",
    "# # Show the extracted data and return the file path if saving is needed\n",
    "# df.head(), accordion_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d49abce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw HTML saved as 'raw_page.html'\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "\n",
    "# ChromeDriver path and Chrome options\n",
    "chromedriver_path = \"/opt/homebrew/bin/chromedriver\"  # Adjust this path based on where your ChromeDriver is installed\n",
    "chrome_options = Options()\n",
    "# chrome_options.add_argument(\"--headless\")  # Run Chrome in headless mode\n",
    "service = Service(chromedriver_path)\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "# Load the webpage\n",
    "url = \"https://stake.com/sports/american-football/usa/nfl/44613931-pittsburgh-steelers-new-york-giants\"\n",
    "driver.get(url)\n",
    "\n",
    "# Give the page some time to fully load (increase if necessary)\n",
    "time.sleep(5)\n",
    "\n",
    "# Get the raw HTML of the page\n",
    "html_content = driver.page_source\n",
    "\n",
    "# Save the raw HTML to a file\n",
    "with open(\"raw_page.html\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(html_content)\n",
    "\n",
    "print(\"Raw HTML saved as 'raw_page.html'\")\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ece04c29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Empty DataFrame\n",
       " Columns: []\n",
       " Index: [],\n",
       " [])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Load the local HTML file\n",
    "html_file_path = 'raw_page2.html'\n",
    "# html_file_path = 'Pittsburgh Steelers VS New York Giants - Stake.com.html'\n",
    "\n",
    "with open(html_file_path, 'r', encoding='utf-8') as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "# Parse the HTML content with BeautifulSoup\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Navigating through layers in the HTML as per the user's instructions\n",
    "content_wrapper = soup.find('div', class_='content-wrapper')\n",
    "\n",
    "if content_wrapper:\n",
    "    # Dive into the nested structure for 'page-content' -> 'layout-spacing' -> 'secondary-accordion'\n",
    "    page_content = content_wrapper.find('div', id='main-content')\n",
    "    layout_spacing = page_content.find('div', class_='layout-spacing') if page_content else None\n",
    "    accordions = layout_spacing.find_all('div', class_='secondary-accordion') if layout_spacing else []\n",
    "\n",
    "    # Collect all text data from the accordions\n",
    "    accordion_data = [accordion.text.strip() for accordion in accordions]\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(accordion_data, columns=['Accordion Content'])\n",
    "\n",
    "    # Display the first few rows\n",
    "    df.head()\n",
    "\n",
    "else:\n",
    "    accordion_data = []\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "# Show the extracted data and return the file path if saving is needed\n",
    "df.head(), accordion_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19e04676",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 42\u001b[0m\n\u001b[1;32m     38\u001b[0m driver\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Step 5: Add a delay to allow the page to load (adjust this if necessary)\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# driver.implicitly_wait(10)\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Step 6: Scrape the content after Cloudflare challenges\u001b[39;00m\n\u001b[1;32m     46\u001b[0m html_content \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mpage_source\n",
      "\u001b[0;31mNameError\u001b[0m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium_stealth import stealth\n",
    "import random\n",
    "\n",
    "# Step 1: Set up user-agent rotation\n",
    "user_agents = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.2 Safari/605.1.15\",\n",
    "    \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:89.0) Gecko/20100101 Firefox/89.0\"\n",
    "    # Add more user agents here\n",
    "]\n",
    "user_agent = random.choice(user_agents)\n",
    "\n",
    "# Step 2: Set up Chrome options\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(f\"user-agent={user_agent}\")\n",
    "\n",
    "# Optional: Use a proxy (residential proxies are recommended for bypassing anti-bot systems)\n",
    "# proxy = \"user:pass@ip:port\"  # Replace with your proxy details\n",
    "# options.add_argument(f'--proxy-server={proxy}')\n",
    "\n",
    "# Initialize the WebDriver with the desired options\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# Step 3: Apply Selenium-stealth settings\n",
    "stealth(driver,\n",
    "        languages=[\"en-US\", \"en\"],\n",
    "        vendor=\"Google Inc.\",\n",
    "        platform=\"Win32\",  # Use 'Win32' to emulate a Windows platform\n",
    "        webgl_vendor=\"Intel Inc.\",\n",
    "        renderer=\"Intel Iris OpenGL Engine\",\n",
    "        fix_hairline=True,  # Fix for detecting thin hairline differences\n",
    "        )\n",
    "\n",
    "# Step 4: Navigate to the page you want to scrape\n",
    "url = \"https://stake.com/sports/american-football/usa/nfl/44613931-pittsburgh-steelers-new-york-giants\"\n",
    "driver.get(url)\n",
    "\n",
    "# Step 5: Add a delay to allow the page to load (adjust this if necessary)\n",
    "# driver.implicitly_wait(10)\n",
    "time.sleep(10)\n",
    "\n",
    "\n",
    "# Step 6: Scrape the content after Cloudflare challenges\n",
    "html_content = driver.page_source\n",
    "\n",
    "# Save the raw HTML to a file\n",
    "with open(\"raw_page.html\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(html_content)\n",
    "\n",
    "print(\"Raw HTML saved as 'raw_page.html'\")\n",
    "\n",
    "# Step 7: Close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e27444b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error clicking the button: Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"/html/body/div[1]/div[2]/div[3]/div[3]/div/div[1]/div/div/div[2]/div/div[1]/div/div/div/div/div[2]/div/div/div/div/div/div[4]/div[1]/div/div/button[1]\"}\n",
      "  (Session info: chrome=130.0.6723.70); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "0   chromedriver                        0x000000010503b9d4 cxxbridge1$str$ptr + 3647524\n",
      "1   chromedriver                        0x0000000105034234 cxxbridge1$str$ptr + 3616900\n",
      "2   chromedriver                        0x0000000104aa010c cxxbridge1$string$len + 88416\n",
      "3   chromedriver                        0x0000000104ae2338 cxxbridge1$string$len + 359308\n",
      "4   chromedriver                        0x0000000104b1bb10 cxxbridge1$string$len + 594788\n",
      "5   chromedriver                        0x0000000104ad6f34 cxxbridge1$string$len + 313224\n",
      "6   chromedriver                        0x0000000104ad7ba4 cxxbridge1$string$len + 316408\n",
      "7   chromedriver                        0x000000010500661c cxxbridge1$str$ptr + 3429484\n",
      "8   chromedriver                        0x0000000105009958 cxxbridge1$str$ptr + 3442600\n",
      "9   chromedriver                        0x0000000104fed344 cxxbridge1$str$ptr + 3326356\n",
      "10  chromedriver                        0x000000010500a21c cxxbridge1$str$ptr + 3444844\n",
      "11  chromedriver                        0x0000000104fde5cc cxxbridge1$str$ptr + 3265564\n",
      "12  chromedriver                        0x0000000105024c98 cxxbridge1$str$ptr + 3554024\n",
      "13  chromedriver                        0x0000000105024e14 cxxbridge1$str$ptr + 3554404\n",
      "14  chromedriver                        0x0000000105033ecc cxxbridge1$str$ptr + 3616028\n",
      "15  libsystem_pthread.dylib             0x0000000185e1b2e4 _pthread_start + 136\n",
      "16  libsystem_pthread.dylib             0x0000000185e160fc thread_start + 8\n",
      "\n",
      "Raw HTML saved as 'raw_page_with_scrapeops.html'\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "\n",
    "# Step 1: Set up Chrome options\n",
    "options = webdriver.ChromeOptions()\n",
    "\n",
    "# Set up ScrapeOps Proxy with API key\n",
    "scrapeops_proxy = \"proxy.scrapeops.io:5353\"\n",
    "api_key = \"08d19650-4b8c-4d3b-83bc-dad61acc9308\"\n",
    "options.add_argument(f'--proxy-server=http://{api_key}:{scrapeops_proxy}')\n",
    "\n",
    "# Optional: Set up user-agent rotation (you can remove or modify this if not needed)\n",
    "user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "options.add_argument(f\"user-agent={user_agent}\")\n",
    "\n",
    "# Initialize the WebDriver with the desired options\n",
    "chromedriver_path = \"/opt/homebrew/bin/chromedriver\"  # Adjust this path based on where your ChromeDriver is installed\n",
    "service = Service(chromedriver_path)\n",
    "options.add_argument(\"--headless\")  # Run Chrome in headless mode\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Step 2: Navigate to the page you want to scrape\n",
    "url = \"https://stake.com/sports/american-football/usa/nfl/44613931-pittsburgh-steelers-new-york-giants\"\n",
    "driver.get(url)\n",
    "\n",
    "# Step 3: Add a delay to allow the page to load (adjust this if necessary)\n",
    "time.sleep(5)  # Adjust this time if necessary\n",
    "\n",
    "# Step 4: Find the first button and click it\n",
    "try:\n",
    "    first_button = driver.find_element(By.XPATH, \"/html/body/div[1]/div[2]/div[3]/div[3]/div/div[1]/div/div/div[2]/div/div[1]/div/div/div/div/div[2]/div/div/div/div/div/div[4]/div[1]/div/div/button[1]\")  # Main button\n",
    "    first_button.click()\n",
    "    print(\"Clicked the first button.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error clicking the button: {e}\")\n",
    "\n",
    "# Step 5: Wait for the page to load after the click\n",
    "time.sleep(5)  # Adjust this time if necessary\n",
    "\n",
    "# Step 6: Get the raw HTML of the page after navigating/clicking\n",
    "html_content = driver.page_source\n",
    "\n",
    "# Step 5: Save the raw HTML to a file\n",
    "with open(\"raw_page_with_scrapeops.html\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(html_content)\n",
    "\n",
    "print(\"Raw HTML saved as 'raw_page_with_scrapeops.html'\")\n",
    "\n",
    "# Step 6: Close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ca6d002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL to scrape\n",
    "url = \"https://stake.com/sports/american-football/usa/nfl/44613931-pittsburgh-steelers-new-york-giants\"\n",
    "\n",
    "# Fetch the page content\n",
    "response = requests.get(url)\n",
    "html_content = response.text\n",
    "\n",
    "# Parse the HTML content with BeautifulSoup\n",
    "soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "# Find all tables in the HTML\n",
    "tables = soup.find_all(\"table\")\n",
    "\n",
    "# Iterate over each table and print its prettified content\n",
    "for index, table in enumerate(tables):\n",
    "    print(f\"Table {index + 1}:\")\n",
    "    print(table.prettify())\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29ac28e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error clicking the button: 'NoneType' object has no attribute 'click'\n"
     ]
    },
    {
     "ename": "NoSuchWindowException",
     "evalue": "Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=130.0.6723.70)\nStacktrace:\n0   chromedriver                        0x000000010545b9d4 cxxbridge1$str$ptr + 3647524\n1   chromedriver                        0x0000000105454234 cxxbridge1$str$ptr + 3616900\n2   chromedriver                        0x0000000104ec010c cxxbridge1$string$len + 88416\n3   chromedriver                        0x0000000104e9bb7c core::str::slice_error_fail::h1cab30ac4b13c655 + 3792\n4   chromedriver                        0x0000000104f285ac cxxbridge1$string$len + 515584\n5   chromedriver                        0x0000000104f3b578 cxxbridge1$string$len + 593356\n6   chromedriver                        0x0000000104ef6f34 cxxbridge1$string$len + 313224\n7   chromedriver                        0x0000000104ef7ba4 cxxbridge1$string$len + 316408\n8   chromedriver                        0x000000010542661c cxxbridge1$str$ptr + 3429484\n9   chromedriver                        0x0000000105429958 cxxbridge1$str$ptr + 3442600\n10  chromedriver                        0x000000010540d344 cxxbridge1$str$ptr + 3326356\n11  chromedriver                        0x000000010542a21c cxxbridge1$str$ptr + 3444844\n12  chromedriver                        0x00000001053fe5cc cxxbridge1$str$ptr + 3265564\n13  chromedriver                        0x0000000105444c98 cxxbridge1$str$ptr + 3554024\n14  chromedriver                        0x0000000105444e14 cxxbridge1$str$ptr + 3554404\n15  chromedriver                        0x0000000105453ecc cxxbridge1$str$ptr + 3616028\n16  libsystem_pthread.dylib             0x0000000185e1b2e4 _pthread_start + 136\n17  libsystem_pthread.dylib             0x0000000185e160fc thread_start + 8\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoSuchWindowException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m3\u001b[39m)  \u001b[38;5;66;03m# Adjust this time if necessary\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Step 6: Get the raw HTML of the page after navigating/clicking\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m html_content \u001b[38;5;241m=\u001b[39m \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpage_source\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Step 7: Save the raw HTML to a file\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_page_after_click.html\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py:455\u001b[0m, in \u001b[0;36mWebDriver.page_source\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpage_source\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    448\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Gets the source of the current page.\u001b[39;00m\n\u001b[1;32m    449\u001b[0m \n\u001b[1;32m    450\u001b[0m \u001b[38;5;124;03m    :Usage:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;124;03m            driver.page_source\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 455\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGET_PAGE_SOURCE\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py:354\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    352\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[0;32m--> 354\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    355\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/selenium/webdriver/remote/errorhandler.py:229\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    227\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[0;32m--> 229\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[0;31mNoSuchWindowException\u001b[0m: Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=130.0.6723.70)\nStacktrace:\n0   chromedriver                        0x000000010545b9d4 cxxbridge1$str$ptr + 3647524\n1   chromedriver                        0x0000000105454234 cxxbridge1$str$ptr + 3616900\n2   chromedriver                        0x0000000104ec010c cxxbridge1$string$len + 88416\n3   chromedriver                        0x0000000104e9bb7c core::str::slice_error_fail::h1cab30ac4b13c655 + 3792\n4   chromedriver                        0x0000000104f285ac cxxbridge1$string$len + 515584\n5   chromedriver                        0x0000000104f3b578 cxxbridge1$string$len + 593356\n6   chromedriver                        0x0000000104ef6f34 cxxbridge1$string$len + 313224\n7   chromedriver                        0x0000000104ef7ba4 cxxbridge1$string$len + 316408\n8   chromedriver                        0x000000010542661c cxxbridge1$str$ptr + 3429484\n9   chromedriver                        0x0000000105429958 cxxbridge1$str$ptr + 3442600\n10  chromedriver                        0x000000010540d344 cxxbridge1$str$ptr + 3326356\n11  chromedriver                        0x000000010542a21c cxxbridge1$str$ptr + 3444844\n12  chromedriver                        0x00000001053fe5cc cxxbridge1$str$ptr + 3265564\n13  chromedriver                        0x0000000105444c98 cxxbridge1$str$ptr + 3554024\n14  chromedriver                        0x0000000105444e14 cxxbridge1$str$ptr + 3554404\n15  chromedriver                        0x0000000105453ecc cxxbridge1$str$ptr + 3616028\n16  libsystem_pthread.dylib             0x0000000185e1b2e4 _pthread_start + 136\n17  libsystem_pthread.dylib             0x0000000185e160fc thread_start + 8\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "# Step 1: Set up Chrome options\n",
    "options = webdriver.ChromeOptions()\n",
    "\n",
    "# Optional: Add a user-agent (you can remove or modify this if not needed)\n",
    "user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "options.add_argument(f\"user-agent={user_agent}\")\n",
    "\n",
    "# Initialize the WebDriver with the desired options\n",
    "chromedriver_path = \"/opt/homebrew/bin/chromedriver\"  # Adjust this path based on where your ChromeDriver is installed\n",
    "service = Service(chromedriver_path)\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Step 2: Navigate to the page you want to scrape\n",
    "url = \"https://stake.com/sports/american-football/usa/nfl/44613931-pittsburgh-steelers-new-york-giants\"\n",
    "driver.get(url)\n",
    "\n",
    "# Step 3: Add a delay to allow the page to load (adjust this if necessary)\n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "# Step 4: Find the \"Main\" button using its class and click it\n",
    "try:\n",
    "    # Use XPath to target the button with the exact text \"Main\"\n",
    "    main_button = driver.find_element(By.XPATH, \"//button[span[text()='Main']]\")\n",
    "    main_button.click()\n",
    "    print(\"Clicked the 'Main' button.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error clicking the button: {e}\")\n",
    "\n",
    "# Step 5: Wait for the page to load after the click\n",
    "time.sleep(3)  # Adjust this time if necessary\n",
    "\n",
    "# Step 6: Get the raw HTML of the page after navigating/clicking\n",
    "html_content = driver.page_source\n",
    "\n",
    "# Step 7: Save the raw HTML to a file\n",
    "with open(\"raw_page_after_click.html\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(html_content)\n",
    "\n",
    "print(\"Raw HTML saved as 'raw_page_after_click.html'\")\n",
    "\n",
    "# Step 8: Close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec98d1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error clicking the button: Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//button[span[text()='Main']]\"}\n",
      "  (Session info: chrome=130.0.6723.70); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "0   chromedriver                        0x00000001030f79d4 cxxbridge1$str$ptr + 3647524\n",
      "1   chromedriver                        0x00000001030f0234 cxxbridge1$str$ptr + 3616900\n",
      "2   chromedriver                        0x0000000102b5c10c cxxbridge1$string$len + 88416\n",
      "3   chromedriver                        0x0000000102b9e338 cxxbridge1$string$len + 359308\n",
      "4   chromedriver                        0x0000000102bd7b10 cxxbridge1$string$len + 594788\n",
      "5   chromedriver                        0x0000000102b92f34 cxxbridge1$string$len + 313224\n",
      "6   chromedriver                        0x0000000102b93ba4 cxxbridge1$string$len + 316408\n",
      "7   chromedriver                        0x00000001030c261c cxxbridge1$str$ptr + 3429484\n",
      "8   chromedriver                        0x00000001030c5958 cxxbridge1$str$ptr + 3442600\n",
      "9   chromedriver                        0x00000001030a9344 cxxbridge1$str$ptr + 3326356\n",
      "10  chromedriver                        0x00000001030c621c cxxbridge1$str$ptr + 3444844\n",
      "11  chromedriver                        0x000000010309a5cc cxxbridge1$str$ptr + 3265564\n",
      "12  chromedriver                        0x00000001030e0c98 cxxbridge1$str$ptr + 3554024\n",
      "13  chromedriver                        0x00000001030e0e14 cxxbridge1$str$ptr + 3554404\n",
      "14  chromedriver                        0x00000001030efecc cxxbridge1$str$ptr + 3616028\n",
      "15  libsystem_pthread.dylib             0x0000000185e1b2e4 _pthread_start + 136\n",
      "16  libsystem_pthread.dylib             0x0000000185e160fc thread_start + 8\n",
      "\n",
      "Raw HTML saved as 'raw_page_after_click.html'\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "# Step 1: Set up Chrome options\n",
    "options = webdriver.ChromeOptions()\n",
    "\n",
    "# Optional: Add a user-agent (you can remove or modify this if not needed)\n",
    "user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "options.add_argument(f\"user-agent={user_agent}\")\n",
    "\n",
    "# Make Chrome run in headless mode\n",
    "options.add_argument(\"--headless\")\n",
    "\n",
    "# Step 2: Set up ScrapeOps Proxy with your API key\n",
    "api_key = '08d19650-4b8c-4d3b-83bc-dad61acc9308'  # Replace with your actual ScrapeOps API key\n",
    "scrapeops_proxy = f\"http://{api_key}:@proxy.scrapeops.io:5353\"\n",
    "options.add_argument(f'--proxy-server={scrapeops_proxy}')\n",
    "\n",
    "# Initialize the WebDriver with the desired options\n",
    "chromedriver_path = \"/opt/homebrew/bin/chromedriver\"  # Adjust this path based on where your ChromeDriver is installed\n",
    "service = Service(chromedriver_path)\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Step 3: Navigate to the page you want to scrape\n",
    "url = \"https://stake.com/sports/american-football/usa/nfl/44613931-pittsburgh-steelers-new-york-giants\"\n",
    "driver.get(url)\n",
    "\n",
    "# Step 4: Add a delay to allow the page to load (adjust this if necessary)\n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "# Step 5: Find the \"Main\" button using its class and click it\n",
    "try:\n",
    "    # Use XPath to target the button with the exact text \"Main\"\n",
    "    main_button = driver.find_element(By.XPATH, \"//button[span[text()='Main']]\")\n",
    "    main_button.click()\n",
    "    print(\"Clicked the 'Main' button.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error clicking the button: {e}\")\n",
    "\n",
    "# Step 6: Wait for the page to load after the click\n",
    "time.sleep(3)  # Adjust this time if necessary\n",
    "\n",
    "# Step 7: Get the raw HTML of the page after navigating/clicking\n",
    "html_content = driver.page_source\n",
    "\n",
    "# Step 8: Save the raw HTML to a file\n",
    "with open(\"raw_page_after_click.html\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(html_content)\n",
    "\n",
    "print(\"Raw HTML saved as 'raw_page_after_click.html'\")\n",
    "\n",
    "# Step 9: Close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e5c0cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe03833a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04e4617b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# ChromeDriver path and Chrome options\n",
    "chromedriver_path = \"/opt/homebrew/bin/chromedriver\"  # Adjust this path based on where your ChromeDriver is installed\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Run Chrome in headless mode\n",
    "service = Service(chromedriver_path)\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "# URL to scrape\n",
    "url = \"https://stake.com/sports/american-football/usa/nfl/44613931-pittsburgh-steelers-new-york-giants\"\n",
    "driver.get(url)\n",
    "\n",
    "# Allow the page to load\n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "# Get the page source\n",
    "html_content = driver.page_source\n",
    "\n",
    "# Parse the HTML content with BeautifulSoup\n",
    "soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "# Find all tables in the HTML\n",
    "tables = soup.find_all(\"table\")\n",
    "\n",
    "# Iterate over each table and print its prettified content\n",
    "for index, table in enumerate(tables):\n",
    "    print(f\"Table {index + 1}:\")\n",
    "    print(table.prettify())\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64604354",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd34e22d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5129ab51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d16b2d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d15c41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aee86af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error clicking the button: Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//button[contains(., 'Main')]\"}\n",
      "  (Session info: chrome=130.0.6723.70); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\n",
      "Stacktrace:\n",
      "0   chromedriver                        0x0000000100ab39d4 cxxbridge1$str$ptr + 3647524\n",
      "1   chromedriver                        0x0000000100aac234 cxxbridge1$str$ptr + 3616900\n",
      "2   chromedriver                        0x000000010051810c cxxbridge1$string$len + 88416\n",
      "3   chromedriver                        0x000000010055a338 cxxbridge1$string$len + 359308\n",
      "4   chromedriver                        0x0000000100593b10 cxxbridge1$string$len + 594788\n",
      "5   chromedriver                        0x000000010054ef34 cxxbridge1$string$len + 313224\n",
      "6   chromedriver                        0x000000010054fba4 cxxbridge1$string$len + 316408\n",
      "7   chromedriver                        0x0000000100a7e61c cxxbridge1$str$ptr + 3429484\n",
      "8   chromedriver                        0x0000000100a81958 cxxbridge1$str$ptr + 3442600\n",
      "9   chromedriver                        0x0000000100a65344 cxxbridge1$str$ptr + 3326356\n",
      "10  chromedriver                        0x0000000100a8221c cxxbridge1$str$ptr + 3444844\n",
      "11  chromedriver                        0x0000000100a565cc cxxbridge1$str$ptr + 3265564\n",
      "12  chromedriver                        0x0000000100a9cc98 cxxbridge1$str$ptr + 3554024\n",
      "13  chromedriver                        0x0000000100a9ce14 cxxbridge1$str$ptr + 3554404\n",
      "14  chromedriver                        0x0000000100aabecc cxxbridge1$str$ptr + 3616028\n",
      "15  libsystem_pthread.dylib             0x0000000185e1b2e4 _pthread_start + 136\n",
      "16  libsystem_pthread.dylib             0x0000000185e160fc thread_start + 8\n",
      "\n"
     ]
    },
    {
     "ename": "NoSuchWindowException",
     "evalue": "Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=130.0.6723.70)\nStacktrace:\n0   chromedriver                        0x0000000100ab39d4 cxxbridge1$str$ptr + 3647524\n1   chromedriver                        0x0000000100aac234 cxxbridge1$str$ptr + 3616900\n2   chromedriver                        0x000000010051810c cxxbridge1$string$len + 88416\n3   chromedriver                        0x00000001004f3b7c core::str::slice_error_fail::h1cab30ac4b13c655 + 3792\n4   chromedriver                        0x00000001005805ac cxxbridge1$string$len + 515584\n5   chromedriver                        0x0000000100593578 cxxbridge1$string$len + 593356\n6   chromedriver                        0x000000010054ef34 cxxbridge1$string$len + 313224\n7   chromedriver                        0x000000010054fba4 cxxbridge1$string$len + 316408\n8   chromedriver                        0x0000000100a7e61c cxxbridge1$str$ptr + 3429484\n9   chromedriver                        0x0000000100a81958 cxxbridge1$str$ptr + 3442600\n10  chromedriver                        0x0000000100a65344 cxxbridge1$str$ptr + 3326356\n11  chromedriver                        0x0000000100a8221c cxxbridge1$str$ptr + 3444844\n12  chromedriver                        0x0000000100a565cc cxxbridge1$str$ptr + 3265564\n13  chromedriver                        0x0000000100a9cc98 cxxbridge1$str$ptr + 3554024\n14  chromedriver                        0x0000000100a9ce14 cxxbridge1$str$ptr + 3554404\n15  chromedriver                        0x0000000100aabecc cxxbridge1$str$ptr + 3616028\n16  libsystem_pthread.dylib             0x0000000185e1b2e4 _pthread_start + 136\n17  libsystem_pthread.dylib             0x0000000185e160fc thread_start + 8\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoSuchWindowException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m3\u001b[39m)  \u001b[38;5;66;03m# Adjust this time if necessary\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Step 6: Get the raw HTML of the page after navigating/clicking\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m html_content \u001b[38;5;241m=\u001b[39m \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpage_source\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Step 7: Save the raw HTML to a file\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_page_after_click.html\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py:455\u001b[0m, in \u001b[0;36mWebDriver.page_source\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpage_source\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    448\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Gets the source of the current page.\u001b[39;00m\n\u001b[1;32m    449\u001b[0m \n\u001b[1;32m    450\u001b[0m \u001b[38;5;124;03m    :Usage:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;124;03m            driver.page_source\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 455\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGET_PAGE_SOURCE\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py:354\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    352\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[0;32m--> 354\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    355\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/selenium/webdriver/remote/errorhandler.py:229\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    227\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[0;32m--> 229\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[0;31mNoSuchWindowException\u001b[0m: Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=130.0.6723.70)\nStacktrace:\n0   chromedriver                        0x0000000100ab39d4 cxxbridge1$str$ptr + 3647524\n1   chromedriver                        0x0000000100aac234 cxxbridge1$str$ptr + 3616900\n2   chromedriver                        0x000000010051810c cxxbridge1$string$len + 88416\n3   chromedriver                        0x00000001004f3b7c core::str::slice_error_fail::h1cab30ac4b13c655 + 3792\n4   chromedriver                        0x00000001005805ac cxxbridge1$string$len + 515584\n5   chromedriver                        0x0000000100593578 cxxbridge1$string$len + 593356\n6   chromedriver                        0x000000010054ef34 cxxbridge1$string$len + 313224\n7   chromedriver                        0x000000010054fba4 cxxbridge1$string$len + 316408\n8   chromedriver                        0x0000000100a7e61c cxxbridge1$str$ptr + 3429484\n9   chromedriver                        0x0000000100a81958 cxxbridge1$str$ptr + 3442600\n10  chromedriver                        0x0000000100a65344 cxxbridge1$str$ptr + 3326356\n11  chromedriver                        0x0000000100a8221c cxxbridge1$str$ptr + 3444844\n12  chromedriver                        0x0000000100a565cc cxxbridge1$str$ptr + 3265564\n13  chromedriver                        0x0000000100a9cc98 cxxbridge1$str$ptr + 3554024\n14  chromedriver                        0x0000000100a9ce14 cxxbridge1$str$ptr + 3554404\n15  chromedriver                        0x0000000100aabecc cxxbridge1$str$ptr + 3616028\n16  libsystem_pthread.dylib             0x0000000185e1b2e4 _pthread_start + 136\n17  libsystem_pthread.dylib             0x0000000185e160fc thread_start + 8\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "# Step 1: Set up Chrome options\n",
    "options = webdriver.ChromeOptions()\n",
    "\n",
    "# Optional: Add a user-agent (you can remove or modify this if not needed)\n",
    "user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "options.add_argument(f\"user-agent={user_agent}\")\n",
    "\n",
    "# Initialize the WebDriver with the desired options\n",
    "chromedriver_path = \"/opt/homebrew/bin/chromedriver\"  # Adjust this path based on where your ChromeDriver is installed\n",
    "service = Service(chromedriver_path)\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Step 2: Navigate to the page you want to scrape\n",
    "url = \"https://stake.com/sports/american-football/usa/nfl/44613931-pittsburgh-steelers-new-york-giants\"\n",
    "driver.get(url)\n",
    "\n",
    "# Step 3: Add a delay to allow the page to load (adjust this if necessary)\n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "# Step 4: Find the first button and click it\n",
    "try:\n",
    "    first_button = driver.find_element(By.XPATH, \"//button[contains(., 'Main')]\")\n",
    "    first_button.click()\n",
    "    print(\"Clicked the first button.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error clicking the button: {e}\")\n",
    "\n",
    "# Step 5: Wait for the page to load after the click\n",
    "time.sleep(3)  # Adjust this time if necessary\n",
    "\n",
    "# Step 6: Get the raw HTML of the page after navigating/clicking\n",
    "html_content = driver.page_source\n",
    "\n",
    "# Step 7: Save the raw HTML to a file\n",
    "with open(\"raw_page_after_click.html\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(html_content)\n",
    "\n",
    "print(\"Raw HTML saved as 'raw_page_after_click.html'\")\n",
    "\n",
    "# Step 8: Close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb2b2d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9f371f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d434a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19d6e13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e6337f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Options' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# ChromeDriver path and Chrome options\u001b[39;00m\n\u001b[1;32m      6\u001b[0m chromedriver_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/opt/homebrew/bin/chromedriver\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Adjust this path based on where your ChromeDriver is installed\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m chrome_options \u001b[38;5;241m=\u001b[39m \u001b[43mOptions\u001b[49m()\n\u001b[1;32m      8\u001b[0m chrome_options\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--headless\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Run Chrome in headless mode\u001b[39;00m\n\u001b[1;32m      9\u001b[0m service \u001b[38;5;241m=\u001b[39m Service(chromedriver_path)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Options' is not defined"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "\n",
    "# ChromeDriver path and Chrome options\n",
    "chromedriver_path = \"/opt/homebrew/bin/chromedriver\"  # Adjust this path based on where your ChromeDriver is installed\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Run Chrome in headless mode\n",
    "service = Service(chromedriver_path)\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "# Open the webpage\n",
    "url = \"https://stake.com/sports/american-football/usa/nfl/44613931-pittsburgh-steelers-new-york-giants\"\n",
    "driver.get(url)\n",
    "\n",
    "# Allow the page to load\n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "# XPath values as provided\n",
    "xpaths = [\n",
    "    \"/html/body/div[1]/div[2]/div[3]/div[3]/div/div[1]/div/div/div[2]/div/div[1]/div/div/div/div/div[2]/div/div/div/div/div/div[4]\",\n",
    "    \"/html/body/div[1]/div[2]/div[3]/div[3]/div/div[1]/div/div/div[2]/div/div[1]/div/div/div/div/div[2]/div/div/div/div/div/div[4]/div[3]\",\n",
    "    \"/html/body/div[1]/div[2]/div[3]/div[3]/div/div[1]/div/div/div[2]/div/div[1]/div/div/div/div/div[2]/div/div/div/div/div/div[4]/div[3]/div\"\n",
    "]\n",
    "\n",
    "# Loop through each XPath and extract the text\n",
    "betting_data = []\n",
    "for xpath in xpaths:\n",
    "    try:\n",
    "        element = driver.find_element(By.XPATH, xpath)\n",
    "        betting_data.append(element.text)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not find element for {xpath}: {e}\")\n",
    "\n",
    "# Convert extracted data into a DataFrame\n",
    "df = pd.DataFrame(betting_data, columns=['Betting Odds'])\n",
    "print(df)\n",
    "\n",
    "# Optionally, save to CSV\n",
    "df.to_csv(\"specific_xpath_betting_odds.csv\", index=False)\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36df9717",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0bdc538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error encountered: Message: \n",
      "Stacktrace:\n",
      "0   chromedriver                        0x00000001006c39d4 cxxbridge1$str$ptr + 3647524\n",
      "1   chromedriver                        0x00000001006bc234 cxxbridge1$str$ptr + 3616900\n",
      "2   chromedriver                        0x000000010012810c cxxbridge1$string$len + 88416\n",
      "3   chromedriver                        0x000000010016a338 cxxbridge1$string$len + 359308\n",
      "4   chromedriver                        0x00000001001a3b10 cxxbridge1$string$len + 594788\n",
      "5   chromedriver                        0x000000010015ef34 cxxbridge1$string$len + 313224\n",
      "6   chromedriver                        0x000000010015fba4 cxxbridge1$string$len + 316408\n",
      "7   chromedriver                        0x000000010068e61c cxxbridge1$str$ptr + 3429484\n",
      "8   chromedriver                        0x0000000100691958 cxxbridge1$str$ptr + 3442600\n",
      "9   chromedriver                        0x0000000100675344 cxxbridge1$str$ptr + 3326356\n",
      "10  chromedriver                        0x000000010069221c cxxbridge1$str$ptr + 3444844\n",
      "11  chromedriver                        0x00000001006665cc cxxbridge1$str$ptr + 3265564\n",
      "12  chromedriver                        0x00000001006acc98 cxxbridge1$str$ptr + 3554024\n",
      "13  chromedriver                        0x00000001006ace14 cxxbridge1$str$ptr + 3554404\n",
      "14  chromedriver                        0x00000001006bbecc cxxbridge1$str$ptr + 3616028\n",
      "15  libsystem_pthread.dylib             0x0000000185e1b2e4 _pthread_start + 136\n",
      "16  libsystem_pthread.dylib             0x0000000185e160fc thread_start + 8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "\n",
    "# ChromeDriver path and Chrome options\n",
    "chromedriver_path = \"/opt/homebrew/bin/chromedriver\"  # Adjust this path based on where your ChromeDriver is installed\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Run Chrome in headless mode\n",
    "service = Service(chromedriver_path)\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "# Load your local HTML file or the URL\n",
    "url = \"https://stake.com/sports/american-football/usa/nfl/44613931-pittsburgh-steelers-new-york-giants\"\n",
    "driver.get(url)\n",
    "\n",
    "# Allow the page to load\n",
    "wait = WebDriverWait(driver, 20)  # Explicit wait up to 20 seconds\n",
    "\n",
    "# Find the main content by following the path step by step using class names\n",
    "try:\n",
    "    # Step 1: Locate the \"content-wrapper\" div\n",
    "    content_wrapper = wait.until(EC.presence_of_element_located((By.CLASS_NAME, \"content-wrapper\")))\n",
    "    \n",
    "    # Step 2: Inside content-wrapper, locate the \"page-content\"\n",
    "    page_content = content_wrapper.find_element(By.ID, \"main-content\")\n",
    "    \n",
    "    # Step 3: Inside \"page-content\", locate the layout spacing container\n",
    "    layout_spacing = page_content.find_element(By.CLASS_NAME, \"layout-spacing\")\n",
    "    \n",
    "    # Step 4: Locate all \"secondary-accordion\" elements within this section\n",
    "    accordions = layout_spacing.find_elements(By.CLASS_NAME, \"secondary-accordion\")\n",
    "    \n",
    "    # Collect all text data from the accordions\n",
    "    accordion_data = [accordion.text for accordion in accordions]\n",
    "    \n",
    "    # Print the data\n",
    "    for index, data in enumerate(accordion_data):\n",
    "        print(f\"Accordion {index + 1}: {data}\")\n",
    "\n",
    "    # Optionally save to CSV\n",
    "    df = pd.DataFrame(accordion_data, columns=['Accordion Content'])\n",
    "    df.to_csv(\"accordion_data.csv\", index=False)\n",
    "    print(\"CSV file saved as 'accordion_data.csv'\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error encountered: {e}\")\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ee895b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "\n",
    "# ChromeDriver path and Chrome options\n",
    "chromedriver_path = \"/opt/homebrew/bin/chromedriver\"  # Adjust this path based on where your ChromeDriver is installed\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Run Chrome in headless mode\n",
    "service = Service(chromedriver_path)\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "# Load your local HTML file\n",
    "local_file_path = \"Scraping/Pittsburgh Steelers VS New York Giants - Stake.com.html\"\n",
    "driver.get(f\"file://{local_file_path}\")\n",
    "\n",
    "# Allow the page to load\n",
    "wait = WebDriverWait(driver, 10)  # Explicit wait up to 20 seconds\n",
    "\n",
    "# Find the main content by following the path step by step using class names\n",
    "try:\n",
    "    # Step 1: Locate the \"content-wrapper\" div\n",
    "    content_wrapper = wait.until(EC.presence_of_element_located((By.CLASS_NAME, \"content-wrapper\")))\n",
    "    \n",
    "    # Step 2: Inside content-wrapper, locate the \"page-content\"\n",
    "    page_content = content_wrapper.find_element(By.ID, \"main-content\")\n",
    "    \n",
    "    # Step 3: Inside \"page-content\", locate the layout spacing container\n",
    "    layout_spacing = page_content.find_element(By.CLASS_NAME, \"layout-spacing\")\n",
    "    \n",
    "    # Step 4: Locate all \"secondary-accordion\" elements within this section\n",
    "    accordions = layout_spacing.find_elements(By.CLASS_NAME, \"secondary-accordion\")\n",
    "    \n",
    "    # Collect all text data from the accordions\n",
    "    accordion_data = [accordion.text for accordion in accordions]\n",
    "    \n",
    "    # Print the data\n",
    "    for index, data in enumerate(accordion_data):\n",
    "        print(f\"Accordion {index + 1}: {data}\")\n",
    "\n",
    "    # Optionally save to CSV\n",
    "    df = pd.DataFrame(accordion_data, columns=['Accordion Content'])\n",
    "    df.to_csv(\"accordion_data.csv\", index=False)\n",
    "    print(\"CSV file saved as 'accordion_data.csv'\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error encountered: {e}\")\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8396fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d24b3a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3ae04a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf883f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
